{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /root/PycharmProjects/test/MNIST_data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /root/PycharmProjects/test/MNIST_data/train-labels-idx1-ubyte.gz\nExtracting /root/PycharmProjects/test/MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting /root/PycharmProjects/test/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# ImportError: No module named input_data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image as Image\n",
    "# Just disables the warning, doesn't enable AVX/FMA\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "\n",
    "# 显示图片\n",
    "def show_bmp(im_arr):\n",
    "    # 参考：http://blog.csdn.net/u010194274/article/details/50817999\n",
    "    im = np.array(im_arr)\n",
    "    im = im.reshape(28, 28)\n",
    "\n",
    "    # fig = plt.figure()\n",
    "    # plotwindow = fig.add_subplot(111)\n",
    "\n",
    "    plt.imshow(im, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_loge(val):\n",
    "    plt.figure(1)\n",
    "    plt.plot(val, np.log(val), 'b--')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 为了用于这个教程，我们使标签数据是\"one-hot vectors\"。 \n",
    "# 一个 one_hot 向量除了某一位的数字是1以外其余各维度数字都是0。\n",
    "# one_hot 标签则是顾名思义，一个长度为n的数组，只有一个元素是1.0，其他元素是0.0。\n",
    "# 所以在此教程中，数字n将表示成一个只有在第n维度（从0开始）数字为1的10维向量。比如，标签0将表示成([1,0,0,0,0,0,0,0,0,0])。\n",
    "mnist = input_data.read_data_sets('/root/PycharmProjects/test/MNIST_data', one_hot=True)\n",
    "\n",
    "# x不是一个特定的值，而是一个占位符placeholder，我们在TensorFlow运行计算时输入这个值。\n",
    "# 我们希望能够输入任意数量的MNIST图像，每一张图展平成784维的向量。\n",
    "# 我们用2维的浮点数张量来表示这些图，这个张量的形状是[None，784 ]。（这里的None表示此张量的第一个维度可以是任何长度的。）\n",
    "with tf.name_scope('inputs'):\n",
    "    x = tf.placeholder(\"float\", [None, 784], name='input_images')\n",
    "    # 为了后面计算交叉熵，我们首先需要添加一个新的占位符用于输入正确值\n",
    "    y_ = tf.placeholder(\"float\", [None, 10], name='correct_values')\n",
    "\n",
    "with tf.name_scope('layer'):\n",
    "    with tf.name_scope('weights'):\n",
    "        # 在这里，我们都用全为零的张量来初始化W和b。因为我们要学习W和b的值，它们的初值可以随意设置。\n",
    "        W = tf.Variable(tf.zeros([784, 10]), name='W')\n",
    "    with tf.name_scope('biases'):  \n",
    "        b = tf.Variable(tf.zeros([10]), name='b')\n",
    "    with tf.name_scope('Wx_plus_b'):\n",
    "        # 首先，我们用tf.matmul(​​X，W)表示x乘以W，对应之前等式里面的，这里x是一个2维张量拥有多个输入。然后再加上b，把和输入到tf.nn.softmax函数里面。\n",
    "        # 至此，我们先用了几行简短的代码来设置变量，然后只用了一行代码来定义我们的模型。\n",
    "        # TensorFlow不仅仅可以使softmax回归模型计算变得特别简单，它也用这种非常灵活的方式来描述其他各种数值计算，从机器学习模型对物理学模拟仿真模型。\n",
    "        # 一旦被定义好之后，我们的模型就可以在不同的设备上运行：计算机的CPU，GPU，甚至是手机！\n",
    "        y = tf.nn.softmax(tf.matmul(x, W) + b, name='y')\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    # 计算交叉熵（越小越好 ，注意 cross_entropy 等于负数，所以 tf.reduce_sum(y_ * tf.log(y)) 越大越好）: \n",
    "    # 交叉熵可在神经网络(机器学习)中作为损失函数，p表示真实标记的分布，q则为训练后的模型的预测标记分布，交叉熵损失函数可以衡量p与q的相似性。\n",
    "    # 交叉熵作为损失函数还有一个好处是使用sigmoid函数在梯度下降时能避免均方误差损失函数学习速率降低的问题，因为学习速率可以被输出的误差所控制。\n",
    "    cross_entropy = -tf.reduce_sum(y_ * tf.log(y), name='cross_entropy')\n",
    "    # 要想明白交叉熵(Cross Entropy)的意义，可以从熵(Entropy) -> KL散度(Kullback-Leibler Divergence) -> 交叉熵这个顺序入手。\n",
    "    # 当然，也有多种解释方法[1]。\n",
    "    # 先给出一个“接地气但不严谨”的概念表述：\n",
    "    #  - 熵：可以表示一个事件A的自信息量，也就是A包含多少信息。\n",
    "    #  - KL散度：可以用来表示从事件A的角度来看，事件B有多大不同。\n",
    "    #  - 交叉熵：可以用来表示从事件A的角度来看，如何描述事件B。\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    # 学习时，使用梯度下降优化器算法，要求 cross_entropy（交叉熵）值最小\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "# 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "writer = tf.summary.FileWriter(\"/root/PycharmProjects/logs\", sess.graph)\n",
    "\n",
    "# # 开始初始化变量\n",
    "# sess.run(init)\n",
    "# \n",
    "# # 用于将 one-hot向量 变为数字（可以忽略）\n",
    "# mask = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "# \n",
    "# for i in range(10000):\n",
    "#     batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "#     if i % 2000 == 0:  # 为了防止打印图片影响性能，则每隔2000个，打印一下图片\n",
    "#         # 显示标签的 one-hot 向量 及其 数字\n",
    "#         print(batch_ys[0], '; 数字=', np.matmul(batch_ys[0], mask).astype(np.uint))\n",
    "#         # 显示数字的图片\n",
    "#         show_bmp(batch_xs[0])\n",
    "#         # 输入 feed_dict 数据，进行学习\n",
    "#     sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "# \n",
    "# # argmax 返回的是最大数的索引.argmax 有一个参数axis=1,表示第1维的最大值.\n",
    "# correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1), name='correct_prediction')\n",
    "# \n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"), name='accuracy')\n",
    "# # 最后，我们计算所学习到的模型在测试数据集上面的正确率。\n",
    "# print('正确率', sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
